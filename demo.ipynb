{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import tempfile\n",
    "from TTS.api import TTS\n",
    "from argparse import Namespace\n",
    "import torch\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "import imageio\n",
    "import glob\n",
    "import pickle\n",
    "import cv2\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from gfpgan import GFPGANer\n",
    "from basicsr.archs.rrdbnet_arch import RRDBNet\n",
    "from realesrgan import RealESRGANer\n",
    "from moviepy.editor import *\n",
    "\n",
    "from musetalk.utils.utils import get_file_type,get_video_fps,datagen\n",
    "from musetalk.utils.preprocessing import get_landmark_and_bbox,read_imgs,coord_placeholder,get_bbox_range\n",
    "from musetalk.utils.blending import get_image\n",
    "from musetalk.utils.utils import load_all_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffmpeg_path = \"./ffmpeg-6.1-amd64-stati\"\n",
    "if ffmpeg_path is None:\n",
    "    print(\"please download ffmpeg-static and export to FFMPEG_PATH. \\nFor example: export FFMPEG_PATH=/musetalk/ffmpeg-4.4-amd64-static\")\n",
    "elif ffmpeg_path not in os.getenv('PATH'):\n",
    "    print(\"add ffmpeg to path\")\n",
    "    os.environ[\"PATH\"] = f\"{ffmpeg_path}:{os.environ['PATH']}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TTSTalker():\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        # Get device\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        # Init TTS\n",
    "        self.tts = TTS(\"tts_models/multilingual/multi-dataset/xtts_v2\").to(device)\n",
    "\n",
    "    def test(self, text, audio, language='en'):\n",
    "\n",
    "        tempf  = tempfile.NamedTemporaryFile(\n",
    "                delete = False,\n",
    "                suffix = ('.'+'wav'),\n",
    "            )\n",
    "\n",
    "        self.tts.tts_to_file(text, speaker_wav=audio, language=language, file_path=tempf.name)\n",
    "\n",
    "        return tempf.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model weights\n",
    "audio_processor,vae,unet,pe  = load_all_model()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "timesteps = torch.tensor([0], device=device)\n",
    "tts_talker = TTSTalker()\n",
    "gfgan_model_path = './gfpgan/weights/GFPGANv1.4.pth'\n",
    "realesrgan_path = './realesrgan/weights/RealESRGAN_x4plus.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_image(file):\n",
    "    pattern = re.compile(r'\\d{8}\\.png')\n",
    "    return pattern.match(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def inference(audio_path, video_path, bbox_shift, enhance_face=False, enhance_background=False):\n",
    "\n",
    "    args_dict={\"result_dir\":'/tmp', \"fps\":24, \"batch_size\":8, \"output_vid_name\":'', \"use_saved_coord\":True}#same with inferenece script\n",
    "    args = Namespace(**args_dict)\n",
    "\n",
    "    input_basename = os.path.abspath(video_path).split('/')[-2]\n",
    "    audio_basename  = os.path.basename(audio_path).split('.')[0]\n",
    "    output_basename = f\"{input_basename}_{audio_basename}\"\n",
    "\n",
    "    result_img_save_path = os.path.join(args.result_dir, output_basename) # related to video & audio inputs\n",
    "\n",
    "    landmarks_save_path = os.path.join(video_path, \"landmarks.pkl\") # only related to video input\n",
    "\n",
    "    os.makedirs(result_img_save_path,exist_ok =True)\n",
    "\n",
    "    output_temp_vid_name = os.path.join(args.result_dir, output_basename+\"_temp.mp4\")\n",
    "    if args.output_vid_name==\"\":\n",
    "        output_vid_name = os.path.join(args.result_dir, output_basename+\".mp4\")\n",
    "    else:\n",
    "        output_vid_name = os.path.join(args.result_dir, args.output_vid_name)\n",
    "\n",
    "    ############################################## extract frames from source video ##############################################\n",
    "    if get_file_type(video_path)==\"video\":\n",
    "        save_dir_full = os.path.join(args.video_path, \"images\")\n",
    "        os.makedirs(save_dir_full,exist_ok = True)\n",
    "\n",
    "        reader = imageio.get_reader(video_path)\n",
    "\n",
    "        for i, im in enumerate(reader):\n",
    "            imageio.imwrite(f\"{save_dir_full}/{i:08d}.png\", im)\n",
    "        input_img_list = sorted(glob.glob(os.path.join(save_dir_full, '*.[jpJP][pnPN]*[gG]')))\n",
    "        fps = get_video_fps(video_path)\n",
    "    else: # input img folder\n",
    "        input_img_list = glob.glob(os.path.join(video_path, '*.[jpJP][pnPN]*[gG]'))\n",
    "        input_img_list = sorted(input_img_list, key=lambda x: int(os.path.splitext(os.path.basename(x))[0]))\n",
    "        fps = args.fps\n",
    "\n",
    "    ############################################## extract audio feature ##############################################\n",
    "    whisper_feature = audio_processor.audio2feat(audio_path)\n",
    "    whisper_chunks = audio_processor.feature2chunks(feature_array=whisper_feature,fps=fps)\n",
    "\n",
    "    ############################################## preprocess input image  ##############################################\n",
    "\n",
    "    if os.path.exists(landmarks_save_path) and args.use_saved_coord:\n",
    "        print(\"using extracted coordinates\")\n",
    "        with open(landmarks_save_path,'rb') as f:\n",
    "            dict = pickle.load(f)\n",
    "            frame_list_cycle = dict[\"frame_list_cycle\"]\n",
    "            coord_list_cycle = dict[\"coord_list_cycle\"]\n",
    "            input_latent_list_cycle = dict[\"input_latent_list_cycle\"]\n",
    "        frame_list = read_imgs(input_img_list)\n",
    "    else:\n",
    "        print(\"extracting landmarks...time consuming\")\n",
    "        coord_list, frame_list = get_landmark_and_bbox(input_img_list, bbox_shift)\n",
    "\n",
    "        i = 0\n",
    "        input_latent_list = []\n",
    "        for bbox, frame in zip(coord_list, frame_list):\n",
    "            if bbox == coord_placeholder:\n",
    "                continue\n",
    "            x1, y1, x2, y2 = bbox\n",
    "            crop_frame = frame[y1:y2, x1:x2]\n",
    "            crop_frame = cv2.resize(crop_frame,(256,256),interpolation = cv2.INTER_LANCZOS4)\n",
    "            latents = vae.get_latents_for_unet(crop_frame)\n",
    "            input_latent_list.append(latents)\n",
    "\n",
    "        # to smooth the first and the last frame\n",
    "        frame_list_cycle = frame_list + frame_list[::-1]\n",
    "        coord_list_cycle = coord_list + coord_list[::-1]\n",
    "        input_latent_list_cycle = input_latent_list + input_latent_list[::-1]\n",
    "\n",
    "        with open(landmarks_save_path, 'wb') as f:\n",
    "            dict = {'frame_list_cycle': frame_list_cycle, 'coord_list_cycle': coord_list_cycle, 'input_latent_list_cycle': input_latent_list_cycle}\n",
    "            pickle.dump(dict, f)\n",
    "\n",
    "    ############################################## inference batch by batch ##############################################\n",
    "    print(\"inferencing talking images...\")\n",
    "    video_num = len(whisper_chunks)\n",
    "    batch_size = args.batch_size\n",
    "    gen = datagen(whisper_chunks,input_latent_list_cycle,batch_size)\n",
    "    res_frame_list = []\n",
    "    for i, (whisper_batch,latent_batch) in enumerate(tqdm(gen,total=int(np.ceil(float(video_num)/batch_size)))):\n",
    "        \n",
    "        tensor_list = [torch.FloatTensor(arr) for arr in whisper_batch]\n",
    "        audio_feature_batch = torch.stack(tensor_list).to(unet.device) # torch, B, 5*N,384\n",
    "        audio_feature_batch = pe(audio_feature_batch)\n",
    "        \n",
    "        pred_latents = unet.model(latent_batch, timesteps, encoder_hidden_states=audio_feature_batch).sample\n",
    "        recon = vae.decode_latents(pred_latents)\n",
    "        for res_frame in recon:\n",
    "            res_frame_list.append(res_frame)\n",
    "\n",
    "    ############################################## pad to full image ##############################################\n",
    "\n",
    "    if enhance_face:\n",
    "        print(\"enhancing talking images...\")\n",
    "        bg_upsampler = None\n",
    "        if enhance_background:\n",
    "            if not torch.cuda.is_available():  # CPU\n",
    "                import warnings\n",
    "                warnings.warn('The unoptimized RealESRGAN is slow on CPU. We do not use it. '\n",
    "                                'If you really want to use it, please modify the corresponding codes.')\n",
    "                bg_upsampler = None\n",
    "            else:\n",
    "\n",
    "                model = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, num_block=23, num_grow_ch=32, scale=2)\n",
    "                bg_upsampler = RealESRGANer(\n",
    "                    scale=2,\n",
    "                    model_path=realesrgan_path,\n",
    "                    model=model,\n",
    "                    tile=400,\n",
    "                    tile_pad=10,\n",
    "                    pre_pad=0,\n",
    "                    half=True)  # need to set False in CPU mode\n",
    "\n",
    "        restorer = GFPGANer(\n",
    "            model_path=gfgan_model_path,\n",
    "            upscale=1,\n",
    "            arch='clean',\n",
    "            channel_multiplier=2,\n",
    "            bg_upsampler=bg_upsampler)\n",
    "\n",
    "    print(\"padding talking image to original video...\")\n",
    "    for i, res_frame in enumerate(tqdm(res_frame_list)):\n",
    "        bbox = coord_list_cycle[i%(len(coord_list_cycle))]\n",
    "        ori_frame = copy.deepcopy(frame_list_cycle[i%(len(frame_list_cycle))])\n",
    "        x1, y1, x2, y2 = bbox\n",
    "        try:\n",
    "            res_frame = cv2.resize(res_frame.astype(np.uint8),(x2-x1,y2-y1))\n",
    "        except:\n",
    "    #                 print(bbox)\n",
    "            continue\n",
    "        \n",
    "        combine_frame = get_image(ori_frame,res_frame,bbox)\n",
    "\n",
    "        if enhance_face:\n",
    "            # gfgan\n",
    "            img = cv2.cvtColor(combine_frame, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "            # restore faces and background if necessary\n",
    "            cropped_faces, restored_faces, r_img = restorer.enhance(\n",
    "                img,\n",
    "                has_aligned=False,\n",
    "                only_center_face=False,\n",
    "                paste_back=True)\n",
    "            \n",
    "            combine_frame = cv2.cvtColor(r_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        cv2.imwrite(f\"{result_img_save_path}/{str(i).zfill(8)}.png\",combine_frame)\n",
    "\n",
    "    images = []\n",
    "    files = [file for file in os.listdir(result_img_save_path) if is_valid_image(file)]\n",
    "    files.sort(key=lambda x: int(x.split('.')[0]))\n",
    "\n",
    "    for file in files:\n",
    "        filename = os.path.join(result_img_save_path, file)\n",
    "        images.append(imageio.imread(filename))\n",
    "\n",
    "    imageio.mimwrite(output_temp_vid_name, images, 'FFMPEG', fps=fps, codec='libx264', pixelformat='yuv420p')\n",
    "\n",
    "    # Check if the input_video and audio_path exist\n",
    "    if not os.path.exists(output_temp_vid_name):\n",
    "        raise FileNotFoundError(f\"Input video file not found: {output_temp_vid_name}\")\n",
    "    if not os.path.exists(audio_path):\n",
    "        raise FileNotFoundError(f\"Audio file not found: {audio_path}\")\n",
    "\n",
    "    # Load the video\n",
    "    video_clip = VideoFileClip(output_temp_vid_name)\n",
    "\n",
    "    # Load the audio\n",
    "    audio_clip = AudioFileClip(audio_path)\n",
    "\n",
    "    # Set the audio to the video\n",
    "    video_clip = video_clip.set_audio(audio_clip)\n",
    "\n",
    "    # Write the output video\n",
    "    video_clip.write_videofile(output_vid_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"It will be good to get back to the Sleeping Lion. After a fortnight going up and down the Still River, chasing a bad lead on a missing blacksmith, you can almost feel the warmth of the inn’s hearth when Gloomhaven’s walls come into view. You are almost home.\"\n",
    "print(os.getcwd)\n",
    "persona_path = './persona/'\n",
    "\n",
    "persona = 'Melanie'\n",
    "reference_video = os.path.join(persona_path, persona, 'images')\n",
    "reference_audio = os.path.join(persona_path, persona, persona + '.mp3')\n",
    "configs = os.path.join(persona_path, persona, 'config.yaml')\n",
    "bbox_shift = OmegaConf.load(configs)['bbox_shift']\n",
    "\n",
    "# Create audio based on text\n",
    "processed_audio = tts_talker.test(input_text, reference_audio)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference(processed_audio, reference_video, bbox_shift)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "musetalk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
